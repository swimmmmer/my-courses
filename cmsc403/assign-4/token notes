debugging
step 1. gcc -std=c99 -g -o Analyzer.out Givens.c Parser.c Tokenizer.c Analyzer.c
step 2. lldb 
step 3. file ./Analyzer.out
step 4. breakpoint set --file Tokenizer.c --line 45 (can add more breakpoints if needed)
step 6. run 
step 7. exit (to exit the debugger and end the program)
after setting breakpoints and running, options i can do: 
1. n (goes to next line by line)
2. print varName (prints the variable called varName e.g. print chara)
3. print (char) chara (prints actual character)
4. c (jumps to next breakpoint)
     -g is for debugging
     -o is for compiling
compiling (compile every time after changes)
gcc -std=c99 -o Analyzer.out Givens.c Parser.c Tokenizer.c Analyzer.c
to run the file without debugging
./Analyzer.out
filename format = ./TokenTests/tokenTest0.txt
./TokenTests/tokenTestBad0.txt


steps:
    1. read character by character
    2. append character to the lexeme we are building
    3. check if the var lexeme is associated with a token
        - might need some function to find the token associated with a lexeme.
        - this function will check if the lexeme = ( then its token is LP, if lexeme = ) then token is RP, etc. 
            for all the different tokens
    4. if the token exists, then save the token somewhere
    5. if the token doesn't exist, then the lexeme cuts one index before and its token is the var token
    6. add the lexeme, token to aLex (array of lexemes, tokens) and increment numLex

Tokenizer.c is not provided and needs to be created. Tokenizer.c will read characters from a given FILE
variable and convert them into tokens. It will do so using a function defined as follows:
_Bool tokenizer(struct lexics *aLex, int *numLex, FILE *inf);
Which takes an array of type lexics, an int pointer representing the number of tokens in the input file, and a
pointer to a FILE. 
The tokenizer function will read characters from the given FILE parameter, creating lexemes
and the associated tokens. Each time a lexeme is generated, a new lexics struct will be created and the lexeme
added. The generated lexeme is then tokenized, the token is added to the generated lexics struct, the lexics struct
is then added to the end of the given lexics array. (Note: another option is to generate lexemes first, then
tokenize the generated lexemes)
The given lexical structure is free format and the location of tokens in the text file does not affect their meaning.
Alphanumeric lexemes will be delimited by both whitespace and by character lexemes. Because character
lexemes are used as delimiters, they cannot be constructed one token at a time. Rather the next several tokens in
the file will need to be examined to determine which (if any) character lexeme is present. (HINT: Because both
whitespace and character lexemes can be delimiters, split functions such as strtok do not provide the needed
functionality and should really be avoided)
The use of helper functions in the Tokenizer.c class is highly recommended. Once the tokenization process is
complete, the tokenizer function should return TRUE. If there occurs an error in the process, the function should
return FALSE.

// Parser.c is not provided and needs to be created. Parser.c will implement a recursive decent parser based upon a provided EBNF grammar. 

// It will do so using a function defined as follows: 
// _Bool parser(struct lexics *someLexics, int numberOfLexics);
// Which takes an array of type lexics and an int representing the number of tokens in the given lexics array. 

// The parser method must take the tokens (given in the array of lexics structs) and determine if they are legal in the language 
// defined by the given grammar. The purpose of our parser is to apply the grammar rules and report any syntax errors. 
// If no syntax errors are identified, parser returns TRUE, otherwise it returns FALSE.

// Parser.c must be a recursive decent predictive parser which utilizes single-symbol lookahead. 
// Parsers which utilize multi-symbol lookahead will not be accepted. If given a grammatically valid input, 
// every token given must be parsed. If a syntax error is found, parsing does not need to continue. 
// Parsers which do not consume every given token for a grammatically valid input will not be accepted
